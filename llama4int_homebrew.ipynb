{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EvolventaAGG/text-generation-webui/blob/main/llama4int_homebrew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, install the CUDA extensions."
      ],
      "metadata": {
        "id": "ZUbS-f1DPXvV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tVeqisMQ5wr"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git\n",
        "%cd 'GPTQ-for-LLaMa'\n",
        "!python setup_cuda.py install\n",
        "#!python test_kernel.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, restart the runtime (but don't delete it). We'll need to do that in order for colab to be able to use the quant_cuda CPP extensions.\n",
        "\n",
        "Afterward, return to this this cell and execute it to clone the repo, install libraries and download your 4 bit LLaMA model of choice."
      ],
      "metadata": {
        "id": "R5-qdqtyPu1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import quant_cuda\n",
        "\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "weights_url = 'https://huggingface.co/decapoda-research/llama-13b-hf-int4/resolve/main/llama-13b-4bit.pt' #@param {type:\"string\"}\n",
        "num_params = \"13b\" #@param [\"7b\", \"13b\", \"30b\", \"65b\"]\n",
        "!wget {weights_url}\n",
        "!pip install git+https://github.com/zphang/transformers@llama_push\n",
        "sys.path.insert(0, '/content/GPTQ-for-LLaMa/')\n",
        "#!CUDA_VISIBLE_DEVICES=0 python llama_inference.py decapoda-research/llama-13b-hf --wbits 4 --load llama-13b-4bit.pt --text \"It was the best of times, it was the worst of times\""
      ],
      "metadata": {
        "id": "a4Q7JnyOZHB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now execute this cell in order to load in the model. Additionally, you can specify your context size (if you're free tier and running 13B, you'll have to keep this pretty low or you may either run out of memory or have ridiculously slow generation times) and a flag denoting whether to load and split the model checkpoint in GPU VRAM before loading (also needed for free tier 13B)."
      ],
      "metadata": {
        "id": "3hBn0BoIQoNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from gptq import *\n",
        "from modelutils import *\n",
        "from quant import *\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "DEV = torch.device('cuda:0')\n",
        "context_size = 1024 #@param {type:\"number\"}\n",
        "split_checkpoint = True #@param {type:\"boolean\"}\n",
        "\n",
        "def load_quant(model, checkpoint, wbits):\n",
        "    from transformers import LLaMAConfig, LLaMAForCausalLM \n",
        "    config = LLaMAConfig.from_pretrained(model)\n",
        "    def noop(*args, **kwargs):\n",
        "        pass\n",
        "    torch.nn.init.kaiming_uniform_ = noop \n",
        "    torch.nn.init.uniform_ = noop \n",
        "    torch.nn.init.normal_ = noop \n",
        "\n",
        "    if split_checkpoint:\n",
        "        print('Splitting checkpoint ...')\n",
        "        ckpt = torch.load(checkpoint, map_location='cuda')\n",
        "\n",
        "        d1 = dict(list(ckpt.items())[:len(ckpt)//2])\n",
        "        torch.save(d1, checkpoint + '0')\n",
        "        del(d1)\n",
        "\n",
        "        d2 = dict(list(ckpt.items())[len(ckpt)//2:])\n",
        "        torch.save(d2, checkpoint + '1')\n",
        "        del(d2)\n",
        "\n",
        "        del(ckpt)\n",
        "\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    transformers.modeling_utils._init_weights = False\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    model = LLaMAForCausalLM(config)\n",
        "    torch.set_default_dtype(torch.float)\n",
        "    model = model.eval()\n",
        "    layers = find_layers(model)\n",
        "    for name in ['lm_head']:\n",
        "        if name in layers:\n",
        "            del layers[name]\n",
        "    make_quant(model, layers, wbits)\n",
        "\n",
        "    if split_checkpoint:\n",
        "        print('Loading model ...')\n",
        "        for i in range(2):\n",
        "            ckpt = torch.load(checkpoint + str(i))\n",
        "            model.load_state_dict(ckpt, strict=False)\n",
        "            del(ckpt)\n",
        "        print('Done.')\n",
        "\n",
        "    else:\n",
        "        ckpt = torch.load(checkpoint)\n",
        "        print('Loading model ...')\n",
        "        model.load_state_dict(torch.load(checkpoint))\n",
        "        print('Done.')\n",
        "\n",
        "    model.seqlen = context_size\n",
        "    return model\n",
        "\n",
        "model = load_quant('decapoda-research/llama-{}-hf'.format(num_params), 'llama-{}-4bit.pt'.format(num_params), 4).cuda()\n",
        "model.to(DEV)\n",
        "tokenizer = AutoTokenizer.from_pretrained('decapoda-research/llama-{}-hf'.format(num_params))"
      ],
      "metadata": {
        "id": "KleSQ3ziiQ3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main GUI."
      ],
      "metadata": {
        "id": "35GvK2M5BASW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import time\n",
        "\n",
        "min_gen_len = 80 #@param {type:\"number\"}\n",
        "max_gen_len = 160 #@param {type:\"number\"}\n",
        "temperature = 1.2 #@param {type:\"number\"}\n",
        "top_p = 0.9 #@param {type:\"number\"}\n",
        "repetition_penalty = 1.1 #@param {type:\"number\"}\n",
        "\n",
        "input_text_area = widgets.Textarea(placeholder='Enter a prompt...',\n",
        "                                   layout=widgets.Layout(width='1200px',\n",
        "                                                         height='600px'))\n",
        "send_button = widgets.Button(description='Send')\n",
        "undo_button = widgets.Button(description='Undo')\n",
        "redo_button = widgets.Button(description='Redo')\n",
        "retry_button = widgets.Button(description='Retry')\n",
        "memory_button = widgets.ToggleButton(description='Memory')\n",
        "\n",
        "hbox = widgets.HBox([input_text_area,\n",
        "                     widgets.VBox([send_button, undo_button, redo_button,\n",
        "                                  retry_button, memory_button])])\n",
        "output = widgets.Output()\n",
        "\n",
        "undo_button.disabled = True\n",
        "redo_button.disabled = True\n",
        "retry_button.disabled = True\n",
        "\n",
        "listen_for_updates = False\n",
        "cur_outputs = []\n",
        "cur_outputs_idx = -1\n",
        "memory_text = ''\n",
        "input_text = ''\n",
        "\n",
        "def generate():\n",
        "    # When creating the context, first, place the full memory followed by a\n",
        "    # newline.\n",
        "    #\n",
        "    # Next, taking the last (max_seq_len-1-max_gen_len-len(mem)) tokens,\n",
        "    # place these tokens in the context.\n",
        "    \n",
        "    if memory_text:\n",
        "        mem_tokenized = tokenizer.encode(memory_text + '\\n', return_tensors='pt')[0].tolist()\n",
        "    else:\n",
        "        mem_tokenized = []\n",
        "    \n",
        "    inp_tokenized = tokenizer.encode(input_text_area.value, return_tensors='pt')[0].tolist()\n",
        "    num_inp_tokens = max(model.seqlen-1-max_gen_len-len(mem_tokenized), 0)\n",
        "\n",
        "    if num_inp_tokens > 0:\n",
        "        tokenized = mem_tokenized + inp_tokenized[-num_inp_tokens:]\n",
        "    elif len(mem_tokenized) > 0:\n",
        "        num_mem_tokens = model.seqlen-1-max_gen_len\n",
        "        tokenized = mem_tokenized[-num_mem_tokens:]\n",
        "    else:\n",
        "        tokenized = []\n",
        "\n",
        "    detokenized = tokenizer.decode(tokenized)\n",
        "    retokenized = tokenizer.encode(detokenized, return_tensors='pt').to(DEV)\n",
        "    prev_num_tokens = len(retokenized[0])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_tokenized = model.generate(retokenized,\n",
        "                                          do_sample=True,\n",
        "                                          min_length=min_gen_len+prev_num_tokens,\n",
        "                                          max_length=max_gen_len+prev_num_tokens,\n",
        "                                          top_p=top_p,\n",
        "                                          temperature=temperature,\n",
        "                                          repetition_penalty=repetition_penalty)[0].tolist()\n",
        "\n",
        "    output = tokenizer.decode(output_tokenized)\n",
        "    num_characters = len(output) - len(detokenized)\n",
        "    return output[-num_characters:]\n",
        "\n",
        "def on_update_input_text_area(change):\n",
        "    global listen_for_updates, cur_outputs, cur_outputs_idx\n",
        "\n",
        "    if listen_for_updates:\n",
        "        cur_outputs = []\n",
        "        cur_outputs_idx = -1\n",
        "        undo_button.disabled = True\n",
        "        redo_button.disabled = True\n",
        "        retry_button.disabled = True\n",
        "\n",
        "def send():\n",
        "    global listen_for_updates, cur_outputs, cur_outputs_idx\n",
        "\n",
        "    input_text_area.disabled = True\n",
        "    memory_button.disabled = True\n",
        "    redo_button.disabled = True\n",
        "    undo_button.disabled = True\n",
        "    retry_button.disabled = True\n",
        "    listen_for_updates = False\n",
        "\n",
        "    generation = generate()\n",
        "    input_text_area.value += generation\n",
        "    cur_outputs_idx += 1\n",
        "    cur_outputs = cur_outputs[:cur_outputs_idx]\n",
        "    cur_outputs.append(generation)\n",
        "\n",
        "    undo_button.disabled = False\n",
        "    retry_button.disabled = False\n",
        "    listen_for_updates = True\n",
        "    memory_button.disabled = False\n",
        "    input_text_area.disabled = False\n",
        "\n",
        "def undo():\n",
        "    global listen_for_updates, cur_outputs, cur_outputs_idx\n",
        "\n",
        "    listen_for_updates = False\n",
        "    num_chars = len(cur_outputs[cur_outputs_idx])\n",
        "    input_text_area.value = input_text_area.value[:-num_chars]\n",
        "    cur_outputs_idx -= 1\n",
        "\n",
        "    if cur_outputs_idx == -1:\n",
        "        undo_button.disabled = True\n",
        "        retry_button.disabled = True\n",
        "    if len(cur_outputs) > 0:\n",
        "        redo_button.disabled = False\n",
        "\n",
        "    listen_for_updates = True\n",
        "\n",
        "def redo():\n",
        "    global listen_for_updates, cur_outputs, cur_outputs_idx\n",
        "\n",
        "    listen_for_updates = False\n",
        "    input_text_area.value += cur_outputs[cur_outputs_idx+1]\n",
        "    cur_outputs_idx += 1\n",
        "\n",
        "    if cur_outputs_idx == len(cur_outputs) - 1:\n",
        "        redo_button.disabled = True\n",
        "    if len(cur_outputs) > 0:\n",
        "        undo_button.disabled = False\n",
        "        retry_button.disabled = False\n",
        "\n",
        "    listen_for_updates = True\n",
        "\n",
        "def send_button_clicked(b):\n",
        "    send()\n",
        "\n",
        "def undo_button_clicked(b):\n",
        "    undo()\n",
        "\n",
        "def redo_button_clicked(b):\n",
        "    redo()\n",
        "\n",
        "def retry_button_clicked(b):\n",
        "    undo()\n",
        "    send()\n",
        "\n",
        "def memory_button_clicked(b):\n",
        "    global listen_for_updates, cur_outputs, cur_outputs_idx, memory_text, \\\n",
        "           input_text\n",
        "    if memory_button.value:\n",
        "        listen_for_updates = False\n",
        "        send_button.disabled = True\n",
        "        undo_button.disabled = True\n",
        "        redo_button.disabled = True\n",
        "        retry_button.disabled = True\n",
        "        input_text = input_text_area.value\n",
        "        input_text_area.value = memory_text\n",
        "    else:\n",
        "        memory_text = input_text_area.value\n",
        "        input_text_area.value = input_text\n",
        "        input_text = ''\n",
        "        send_button.disabled = False\n",
        "        undo_button.disabled = cur_outputs_idx < 0\n",
        "        redo_button.disabled = cur_outputs_idx >= len(cur_outputs) - 1\n",
        "        retry_button.disabled = undo_button.disabled\n",
        "        listen_for_updates = True\n",
        "\n",
        "send_button.on_click(send_button_clicked)\n",
        "undo_button.on_click(undo_button_clicked)\n",
        "redo_button.on_click(redo_button_clicked)\n",
        "retry_button.on_click(retry_button_clicked)\n",
        "memory_button.observe(memory_button_clicked, names='value')\n",
        "input_text_area.observe(on_update_input_text_area, names='value')\n",
        "\n",
        "display(hbox, output)"
      ],
      "metadata": {
        "id": "u-VDvKNqav-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}